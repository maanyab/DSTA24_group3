# Milestone 2 Report 

*DSTA24 Fall Term 2024 - Group 3*

__*Authors*__
- *Vishal Alluri* 
- *Maanya Bagga*
- *Tommaso Fruci*


## Task 1
#### Content of the .gitignore file

#### Byte-compiled files

This section contains files generated by Python during the execution of the scripts. These files are not needed because they can be recreated from the original .py files.

##### Environments

This section contains virtual environments and directories that store dependencies and runtime information. Since these informations are machine-specific we don't need them.

#### Files

- NumPy array files are files that can be regenerated from the original datasets. So we don't need these binary data files (that are often large)

- CSV files can also be often regenerated and are often too large to include in the repository.

- Tab-separated files are excluded for the same reason.

- SQLite database files contain local data that can also be regenerated or are too large.

- .keras and .h5 have been added to avoid storing trained models, that can be very large files which would slow down operations in the repository. Since these files are binary any modification would imply storing an entire new file instead of checking for small differences in the pre-existing version.

#### System files
This section contains files created by operating systems that have no relevance to the project's functionality.

#### Logs and Debug files

These are files created during the program execution fro debugging purposes. Since they are temporary files they don't need to be version-controlled.

#### Media files

Images, if necessary, can be stored in separate repositories or special media storage solution. Video and audio files are ecluded due to their typically large size.


#### Sharing changes in the .gitignore file

We created a new branch called ".gitignore-updates" to handle all the updates of the .gitignore file.
Any change to the file must be made on this new branch to make sure that we have a single reference for the composition of the file.

Whenever a group member need the updated .gitignore file in their feature branch, they can merge the latest version of the ".gitignore-updates" branch into their personal branch. This approach allows everyone to work with the updated versions of the file without needing to create pull requests into the main branch for every small change.

When all the tasks are completed the ".gitignore-updates" branch will be already integrated in the feature branches, so the main branch will have the updated .gitignore file when the feature branches are merged into main.

## Task 2
#### Hash functions

A hash function is a mathematical function that takes an input and creates a fixed-lenght string of characters. 
The input can be a string of any size (for example a word, a sentence, a digital picture) and the output will be a fixed-lenght bit-string (SHA256 has an output of 256 bits).

A characteristic of hash functions is that they are deterministic, this means that the same input always gives the same output. Also given a hash value (output) it should be infeasible to find any input that gives the same output (hash functions are one-way functions). It should also be infeasible to find two different inputs that produce the same outputs (collision-resistance): collision must exist, since the set of possible inputs is larger than the set of possible outputs (infeasible means that it would be too time consuming with today's knowledge and computing power).

Some of the use cases of hash functions are:

- Ensure data integrity when sending data to someone: the computer can compare the two hashes of the data rather than comparing the entire data in the original form (which is larger) to make sure that the informations have not been altered after been created.

- Ensure that communication is secure thorugh the use of digital signatures, to verify the authenticity and integrity of messages or digial documents.

- Hash functions can convert data into unique codes, simplifying the process of finding or checking informations. This is often used to speed up data searches or in the context of password security.


#### Python module, package and script

A Python module is a .py file that contains reusable code. Modules can be imported and used in other Python files.

A Python package is a group of modules organized in a directory that contains an "__init__.py" file. This file signals to Python that the directory in which it is contained should be considered as a package.

A Python script is a Python file created to be executed directly and used for a specific task or program.

#### Docker

To explain what a Docker container is to a child we can refer to is as a special portable isolated room that has all the toys inside and can be taken everywhere.

To explain what a Docker volume is we can describe it as a cabinet with all the toys inside, and even if the special room (the container) is deleted or replaced, this cabinet stays in place with all the toys ready to use and safe.


#### Virtual Env or Docker

I've never used Docker until now, but I understant the benefits of using it. 
I would use virtualenv for small projects to ensure dependencies isolation and avoid conflicts with system-installed packages. 
I would choose Docker for large projects that require environments across different systems or project that require more than one component to work together (ike a database and an application running at the same time).


#### Docker build context

The Docker build context is the folder you choose when building a Docker image using the 'docker build' command. It includes all the files contained in that directory that are sent to Docker to create the image.

For example the 'Dockerfile' and all the files it needs must be in this folder. To avoid sending files that are not necessary (that will slow down the build process) a '.dockerignore' file can be added.


#### Assess the quality of a Python package on PyPI

To assess the quality of a Python package on PyPI, I would check if it has good documentation with clear installation instructions and examples. Then look at how often the package is updated and whether the maintainers are active in addressing issues. 
I would consider how popular the package is by checking if it has a lot of downloads or good ratings on GitHub. I also check for signs of reliability, looking for things like automated tests or "build passing" labels, which show that the package is being tested and works correctly. If I can access the code, I check if it’s clean and easy to understand. I also make sure it doesn’t rely on old or unsafe tools, which could cause problems.


## Task 3 

Installed the required packages to run the code file. However, encountered the "Timeout Error" while installing the Tensorflow package. 
Updated pip install as suggested by the output using the command below

``` pip install --upgrade pip```

After upgrading, installed TensorFlow again but did still encountered the same error(I did this while in the library).
Tried it again at my home network and this time it worked.

**Load and Train the model**
The existing code already loads the data and trains the model.

**Save a fitted model to saved model type**
As the version of tensorflow was 2.16, therefore I saved the file with saved model type. However, I encountered a ValueError stating that the save_format is deprecated in Keras 3. Thereafter I removed this argument as suggested in the error message and proceeded with passing the file name with '.keras' extension.
While trying to save the model, I ran into Attribute error as I accidently saved model history instead of model itself.

**load a saved model type**
Encountered a ValueError while loading the file as SavedModel format is not supported by ```load_model()``` in Keras 3. Added '.keras' extension to the file name

**Perform predictions using keras**
While researching I read that it is good practice to check if predictions remain the same after we save and reload the model. Hence, included a code that verifies the same. The ```np.testing.assert_allclose``` raises an assertion error if the predictions are too dissimilar. No assertion error was encountered and first five predictions using the fitted model were printed. 

As the file would be too large for Git Repository before commiting it was added to .gitignore


## Task 4 

Our aim was to put particular definitions in separate files so that the functionalities can be used for various other programs as well. The manner in which we broke our code down will also make it easier to collaborate among our team members in the future as we can work on particular modules independently. We modulised our code keeping in mind whether or not two functions are usually used together, if so then they go in the same file. For instance, if we are loading the data, we would have to do some preprocessing. Therefore, both functionalities are included in the same file. Lastly, the intent was to structure the code in the way that it has a number of modular functions so that we can test each function individually to ensure the code works smoothly (unit testing), instead of waiting for the whole code to run.

The table below shows code breakdown into modules -

| **File/Module Name**    |**Functionality** |
|------------------|------------------|
|data_handling.py               | This Module loads and preprocesses the data. If same data is to be used for different models this ensures consistency.|  
|neuralnet_architecture.py      | This module handles the building of neural network model. Putting it in a separate function ensures that we can make changes to model without affecting the training logic.|
|train_eval.py                  | contains the function for training and evaluating the trained model.
|saving_FittedModel.py          | saves the fitted model with .keras extensiona and loads the fitted model
|predicting.py                  | Module to makes predictions using the saved model
|main.py                        | Combines the overall workflow by importing functions from the above modules. Serves as the single entry point to execute the pipeline (python3 main.py).


**Problems**
After I made separate python files for each module, I received a Pylance error likely because the Python interpreter selected in VS code was one where the dependencies were not installed. Hence, I opened the command palette in VSCode, went to the python selector and selected the correct virtual environment. Thereafter, the modularised code ran smoothly.

## Task 5

After installing all the dependencies and making sure the code runs smoothly, 

- we created a requirements.txt file using 
''' pip freeze > requirements.txt```. 
and an environment.yml file using 
```
conda env export > environment.yml
```
The file included the dependencies of our code as well as their respective versions.

- To test the whether our requirements file works smoothly, we created and activated a new virtual environment on a **new machine** using the command 
```
python3 -m venv venv
source venv/bin/activate
```
- After activating, we installed the depedencies from the requirements.txt file using the command
```
pip install -r requirements.txt
```
- Thereafter, we ran our code script main.py.

However, we ran into issues with this. As the code did not run on the new machine as it was being run on the virtual machine. However, when the code was executed on a a machine with different Operating System but outside Virtual machine. The code seemed to work. 

**Alternative Approach**

- We also tested whether the requirements file works on the **same machine but a fresh virtual environment**
- We began by deactivating and removing the virtual environment in which the code was initially written
```
deactivate
rm -rf venv
```
- Thereafter creating a new conda environment with a python version of 3.11, followed by activating the virtual environment and installing all the dependencies. This time the code worked.
```
conda create -n myenv python=3.11
conda activate myenv

#Install dependencies in the virtual environment with pip
pip install -r DSTA24_group3/requirements.txt

# or 

#Install dependencies in the virtual environment with Conda
conda env create -f DSTA24_group3/environment.yml
conda activate DSTA1_env

#Run the code
python3 DSTA24_group3/src/main.py
```
## Task 6
- To perform task 6, a new branch called “Task6” was created with “Maanya_milestone2” branch as base, since it contained the necessary code modularization. The environment was recreated using the requirement.txt file.
- To test, the code was run but always led to “Illegal instruction (code dumped error)”. One thought-out fix was to use pyenv to use the exact python version as the previous author, but even this did not solve it. The next idea was to work with environment.yml files which is based on anaconda, but still led to the same error.
-  Simultaneously, Docker desktop was installed, although the installation was correct, it seemed to run into the error of not having KVM support for VM which  did not allow visualization. VM’s processor settings were changed according to recommendations online but did not give a positive result. Hours were spent making no progress, finally based on a quick discussion with groupmate, it was decided that the Linux VM might be incompatible and to use Windows machine, this solved the issue.

# Dockerization:
-  Docker was first installed on the Windows machine. A new environment was created using the and the necessary dependencies were installed based on the requirements.txt file on the new machine. A .dockerignore file was created to exclude unnecessary files like __pycache__, .venv, and .git from the Docker build context.
- 
-  Then the existing script of saving_FittedModel.py was modified to ensure that the trained model is saved and loaded from the Docker-compatible directory ```/app/model```. The file ```main.py```  was updated to integrate Docker-compatible paths.

-  For the Dockerfile, the latest official TensorFlow Docker image was used as the base image:
```tensorflow/tensorflow:latest”.```
-  The working directory was set to /app and instructed to copy source files into the container. To save the trained model outside the container, a volume was used ```/app/model```. Finally the default command was to run ```main.py```.
  
-  The Docker image was built using the following command:
```docker build -t neuralnet_dockerized .```

- Then the container was run while mapping the host directory for model persistence:
```docker run -it --rm -v "$(pwd)/model:/app/model" neuralnet_dockerized```



## Appendix 

The following table presents the python packages used along with their versions and SHA256 hash digest

| **Package Name** | **Version** | **SHA256 Hash Digest** |
|------------------|-------------|------------------------|
|absl-py           |       2.1.0 | 526a04eadab8b4ee719ce68f204172ead1027549089702d99b9059f129ff1308                   |
|astunparse        |1.6.3        | 526a04eadab8b4ee719ce68f204172ead1027549089702d99b9059f129ff1308
|certifi           |2024.8.30    | 922820b53db7a7257ffbda3f597266d435245903d80737e34f8a45ff3e3230d8
|charset-normalizer| 3.4.0       | 922820b53db7a7257ffbda3f597266d435245903d80737e34f8a45ff3e3230d8
|flatbuffers       | 24.3.25     |   922820b53db7a7257ffbda3f597266d435245903d80737e34f8a45ff3e3230d8
|gast              |0.6.0        | 922820b53db7a7257ffbda3f597266d435245903d80737e34f8a45ff3e3230d8   
|google-pasta     |    0.2.0     | b32482794a366b5366a32c92a9a9201b107821889935a02b3e51f6b432ea84ed
|grpcio            | 1.68.0| b32482794a366b5366a32c92a9a9201b107821889935a02b3e51f6b432ea84ed
|h5py|3.12.1|b32482794a366b5366a32c92a9a9201b107821889935a02b3e51f6b432ea84ed
|idna|3.10| b32482794a366b5366a32c92a9a9201b107821889935a02b3e51f6b432ea84ed
|keras|3.6.0|b32482794a366b5366a32c92a9a9201b107821889935a02b3e51f6b432ea84ed
|libclang|18.1.1|b32482794a366b5366a32c92a9a9201b107821889935a02b3e51f6b432ea84ed
|Markdown|3.7|b32482794a366b5366a32c92a9a9201b107821889935a02b3e51f6b432ea84ed
|markdown-it-py|3.0.0|b32482794a366b5366a32c92a9a9201b107821889935a02b3e51f6b432ea84ed
|MarkupSafe|3.0.2|b32482794a366b5366a32c92a9a9201b107821889935a02b3e51f6b432ea84ed
|mdurl|0.1.2|b32482794a366b5366a32c92a9a9201b107821889935a02b3e51f6b432ea84ed
|ml-dtypes|0.3.2|b32482794a366b5366a32c92a9a9201b107821889935a02b3e51f6b432ea84ed
|namex|0.0.8|b32482794a366b5366a32c92a9a9201b107821889935a02b3e51f6b432ea84ed
|numpy|1.26.4|b32482794a366b5366a32c92a9a9201b107821889935a02b3e51f6b432ea84ed
|opt_einsum|3.4.0|b32482794a366b5366a32c92a9a9201b107821889935a02b3e51f6b432ea84ed
|optree|0.13.1|b32482794a366b5366a32c92a9a9201b107821889935a02b3e51f6b432ea84ed
|packaging|24.2|09abb1bccd265c01f4a3aa3f7a7db064b36514d2cba19a2f694fe6150451a759
|protobuf|4.25.5|09abb1bccd265c01f4a3aa3f7a7db064b36514d2cba19a2f694fe6150451a759
|Pygments|2.18.0|09abb1bccd265c01f4a3aa3f7a7db064b36514d2cba19a2f694fe6150451a759
|requests|2.32.3|09abb1bccd265c01f4a3aa3f7a7db064b36514d2cba19a2f694fe6150451a759
|rich|13.9.4|6049d5e6ec054bf2779ab3358186963bac2ea89175919d699e378b99738c2a90
|setuptools|75.5.0|6049d5e6ec054bf2779ab3358186963bac2ea89175919d699e378b99738c2a90
|six|1.16.0|6049d5e6ec054bf2779ab3358186963bac2ea89175919d699e378b99738c2a90
|tensorboard|2.16.2|6049d5e6ec054bf2779ab3358186963bac2ea89175919d699e378b99738c2a90
|tensorboard-data-server|0.7.2|6049d5e6ec054bf2779ab3358186963bac2ea89175919d699e378b99738c2a90
|tensorflow|2.16.2|6049d5e6ec054bf2779ab3358186963bac2ea89175919d699e378b99738c2a90
|termcolor|2.5.0|6049d5e6ec054bf2779ab3358186963bac2ea89175919d699e378b99738c2a90
|typing_extensions|4.12.2|6049d5e6ec054bf2779ab3358186963bac2ea89175919d699e378b99738c2a90
|urllib3|2.2.3|6049d5e6ec054bf2779ab3358186963bac2ea89175919d699e378b99738c2a90
|Werkzeug|3.1.3|6049d5e6ec054bf2779ab3358186963bac2ea89175919d699e378b99738c2a90
|wheel|0.45.0|6049d5e6ec054bf2779ab3358186963bac2ea89175919d699e378b99738c2a90
|wrapt|1.16.0|6049d5e6ec054bf2779ab3358186963bac2ea89175919d699e378b99738c2a90
